{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1147d0e7-d876-41a9-ba96-fa0e9dbba802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.detection import find_peaks\n",
    "from photutils.segmentation import detect_sources\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "def reduce_science_frame(\n",
    "    science_filename,\n",
    "    median_bias_filename,\n",
    "    median_flat_filename,\n",
    "    median_dark_filename,\n",
    "    reduced_science_filename=\"reduced_science.fits\",\n",
    "):\n",
    "    \"\"\"Reduce a science frame using bias, dark, and flat frames.\"\"\"\n",
    "\n",
    "    # Load all data\n",
    "    with fits.open(science_filename) as hdul:\n",
    "        science_header = hdul[0].header\n",
    "        exposure_time = science_header.get('EXPTIME')\n",
    "        science_data = fits.getdata(science_filename).astype('f4')\n",
    "\n",
    "    bias_data = fits.getdata(median_bias_filename).astype('f4')\n",
    "    dark_data = fits.getdata(median_dark_filename).astype('f4')\n",
    "    flat_data = fits.getdata(median_flat_filename).astype('f4')\n",
    "    \n",
    "\n",
    "    # Subtract bias\n",
    "    reduced_science = science_data - bias_data\n",
    "\n",
    "    # Subtract dark (scaled by exposure time)\n",
    "    reduced_science -= dark_data * exposure_time\n",
    "\n",
    "    # Flat field correction\n",
    "    reduced_science /= flat_data\n",
    "\n",
    "    # Optional: remove cosmic rays (simple median filter + threshold method)\n",
    "    # This can be replaced with astroscrappy or similar for better accuracy\n",
    "    smoothed = median_filter(reduced_science, size=3)\n",
    "    residual = reduced_science - smoothed\n",
    "    threshold = 5 * np.std(residual)\n",
    "    mask = residual > threshold\n",
    "    reduced_science[mask] = smoothed[mask]  # Replace cosmic rays\n",
    "\n",
    "    # Save to FITS\n",
    "    primary_science = fits.PrimaryHDU(data=reduced_science.data, header=fits.Header())\n",
    "    primary_science.writeto(reduced_science_filename, overwrite=True)\n",
    "\n",
    "    return reduced_science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "528894c1-ccbe-4930-aaf2-f8abbeb81bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits \n",
    "from astropy.stats import sigma_clip\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "def create_median_bias(bias_list, median_bias_filename):\n",
    "    \"\"\"Creates a median bias frame from a list of FITS bias files.\n",
    "\n",
    "    Parameters:\n",
    "    - bias_list: list of strings, paths to bias FITS files.\n",
    "    - median_bias_filename: string, path to save the resulting median bias frame.\n",
    "\n",
    "    Returns:\n",
    "    - A 2D numpy array of the median bias frame.\n",
    "    \"\"\"\n",
    "    # Read all bias frames into a 3D numpy array (stack of 2D images)\n",
    "    bias_frames = []\n",
    "    for file in bias_list:\n",
    "        data = fits.getdata(file).astype('f4')\n",
    "        bias_frames.append(data)\n",
    "        \n",
    "    bias_stack = np.array(bias_frames)\n",
    "    \n",
    "    # Apply sigma clipping along the stack axis (axis=0)\n",
    "    clipping = sigma_clip(bias_stack, cenfunc = 'median', sigma=3.0, axis=0)\n",
    "    \n",
    "\n",
    "    # Take the median of the clipped data\n",
    "    # Convert masked array to regular ndarray before saving\n",
    "    median_bias = np.ma.median(clipping, axis=0)\n",
    "    if isinstance(median_bias, np.ma.MaskedArray):\n",
    "        median_bias = median_bias.filled()\n",
    "    # returns fits immages \n",
    "    primary = fits.PrimaryHDU(data=median_bias, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_bias_filename, overwrite=True)\n",
    "\n",
    "    return median_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f4037e-35f9-45df-a505-26529c1d4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# @Filename: darks.py\n",
    "# @License: BSD 3-clause (http://www.opensource.org/licenses/BSD-3-Clause)\n",
    "\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "\n",
    "def create_median_dark(dark_list, bias_filename, median_dark_filename):\n",
    "    \"\"\"Creates a median dark frame corrected for bias and normalized by exposure time.\n",
    "\n",
    "    Parameters:\n",
    "    - dark_list: list of strings, paths to dark FITS files.\n",
    "    - bias_filename: string, path to the median bias FITS file.\n",
    "    - median_dark_filename: string, path to save the resulting median dark frame.\n",
    "\n",
    "    Returns:\n",
    "    - A 2D numpy array of the median dark frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the median bias frame\n",
    "    bias_data = fits.getdata(bias_filename).astype('f4')\n",
    "\n",
    "    # List to store bias-subtracted and normalized dark frames\n",
    "    dark_corrected_frames = []\n",
    "\n",
    "    for path in dark_list:\n",
    "        with fits.open(path) as dark:\n",
    "            dark_data = dark[0].data.astype('f4')\n",
    "            exptime = dark[0].header['EXPTIME']\n",
    "            if exptime <= 0:\n",
    "                raise ValueError(f\"Invalid EXPTIME ({exptime}) in {path}\")\n",
    "            dark_nobias = dark_data - bias_data\n",
    "            dark_corrected_frames.append(dark_nobias / exptime)\n",
    "            header = dark[0].header.copy()\n",
    "\n",
    "\n",
    "    # Stack and sigma clip the corrected dark frames\n",
    "    dark_3d = np.array(dark_corrected_frames)\n",
    "    clipping = sigma_clip(dark_3d, cenfunc = 'median', sigma=3.0, axis=0)\n",
    "\n",
    "    # Compute the median of the clipped data\n",
    "    median_dark = np.ma.median(clipping, axis=0).filled()\n",
    "\n",
    "    # Save to a FITS file\n",
    "    dark_hdu = fits.PrimaryHDU(data=median_dark, header=fits.Header())\n",
    "    dark_hdu.writeto(median_dark_filename, overwrite=True)\n",
    "\n",
    "    return median_dark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d5a53a8-8188-40b5-9fdf-cc4a86a00d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# @Filename: flats.py\n",
    "# @License: BSD 3-clause (http://www.opensource.org/licenses/BSD-3-Clause)\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_median_flat(\n",
    "    flat_list,\n",
    "    bias_filename,\n",
    "    median_flat_filename,\n",
    "    dark_filename=None,\n",
    "):\n",
    "    \"\"\"This function creates a normalized median flat field from a list of flat frames.\"\"\"\n",
    "\n",
    "    # Load the bias frame\n",
    "    bias_data = fits.getdata(bias_filename).astype('f4')\n",
    "\n",
    "    # Optionally load dark frame\n",
    "    dark_data = fits.getdata(dark_filename).astype('f4') if dark_filename else None\n",
    "\n",
    "    corrected_flats = []\n",
    "\n",
    "    # Use the filter of the first flat frame as the reference\n",
    "    with fits.open(flat_list[0]) as hdul:\n",
    "        reference_filter = hdul[0].header.get('FILTER', '')\n",
    "\n",
    "    for file in flat_list:\n",
    "        with fits.open(file) as hdul:\n",
    "            flat_data = fits.getdata(file).astype('f4')\n",
    "            header = hdul[0].header\n",
    "\n",
    "            # Check that the filters match\n",
    "            flat_filter = header.get('FILTER', '')\n",
    "            if flat_filter != reference_filter:\n",
    "                raise ValueError(f\"Filter mismatch in {flat_filter}: expected {reference_filter}, got {flat_filter}\")\n",
    "\n",
    "            # Subtract bias\n",
    "            corrected = flat_data - bias_data\n",
    "\n",
    "            # Optionally subtract scaled dark\n",
    "            if dark_data is not None:\n",
    "                flat_exptime = header.get('EXPTIME', 1.0)\n",
    "                corrected -= dark_data * flat_exptime\n",
    "\n",
    "            corrected_flats.append(corrected)\n",
    "\n",
    "    # Stack all corrected flats\n",
    "    flat_stack = np.stack(corrected_flats)\n",
    "\n",
    "    # Sigma clipping and median combine\n",
    "    clipped = sigma_clip(flat_stack, cenfunc=\"median\", sigma=3.0, axis=0)\n",
    "    median_flat = np.ma.median(clipped, axis=0).filled(np.nan)\n",
    "    \n",
    "\n",
    "    # Normalize the flat\n",
    "    normalized_flat = median_flat / np.nanmedian(median_flat)\n",
    "\n",
    "    # Save to FITS\n",
    "    header = fits.getheader(flat_list[0])\n",
    "    header['HISTORY'] = 'Bias and optional dark subtracted, sigma-clipped median combined, and normalized.'\n",
    "    hdu = fits.PrimaryHDU(data=normalized_flat, header=header)\n",
    "    hdu.writeto(median_flat_filename, overwrite=True)\n",
    "\n",
    "    return normalized_flat\n",
    "\n",
    "\n",
    "def plot_flat(\n",
    "    median_flat_filename,\n",
    "    ouput_filename=\"median_flat.png\",\n",
    "    profile_ouput_filename=\"median_flat_profile.png\",\n",
    "):\n",
    "    \"\"\"This function plots a normalized flat field image and its median profile.\"\"\"\n",
    "\n",
    "    # Load the flat frame\n",
    "    flat = fits.getdata(median_flat_filename).astype('f4')\n",
    "\n",
    "    # Plot the 2D flat field\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    vmin, vmax = 0.9, 1.1\n",
    "    plt.imshow(flat, cmap='gray', origin='lower', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label='Normalized Intensity')\n",
    "    plt.title('Normalized Median Flat')\n",
    "    plt.xlabel('X Pixel')\n",
    "    plt.ylabel('Y Pixel')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ouput_filename)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the 1D median profile along the y-axis\n",
    "    profile = np.median(flat, axis=0)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(profile, lw=1.5)\n",
    "    plt.title('Median Flat Profile (along Y-axis)')\n",
    "    plt.xlabel('X Pixel')\n",
    "    plt.ylabel('Median Normalized Value')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(profile_ouput_filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0f46601-b2cb-4d46-8f07-936a999d07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
    "from astropy.table import vstack\n",
    "\n",
    "\n",
    "def do_aperture_photometry(\n",
    "    image,\n",
    "    positions,\n",
    "    radii,\n",
    "    sky_radius_in,\n",
    "    sky_annulus_width,\n",
    "):\n",
    "    \"\"\"Perform aperture photometry at given positions and radii, with sky background subtraction.\"\"\"\n",
    "\n",
    "    # Handle image input: filename or NumPy array\n",
    "    if isinstance(image, str) or isinstance(image, pathlib.Path):\n",
    "        with fits.open(image) as hdul:\n",
    "            data = hdul[0].data.astype('f4')\n",
    "    else:\n",
    "        data = image.astype('f4')\n",
    "\n",
    "    # Store results\n",
    "    all_results = []\n",
    "\n",
    "    for radius in radii:\n",
    "\n",
    "        aperture = CircularAperture(positions, r=radius)\n",
    "        annulus = CircularAnnulus(positions, r_in=sky_radius_in, r_out=sky_radius_in + sky_annulus_width)\n",
    "\n",
    "        aper_phot = aperture_photometry(data, aperture)\n",
    "        bkg_phot = aperture_photometry(data, annulus)\n",
    "\n",
    "        annulus_area = annulus.area\n",
    "        aperture_area = aperture.area\n",
    "        bkg_mean = bkg_phot['aperture_sum'] / annulus_area\n",
    "        bkg_total = bkg_mean * aperture_area\n",
    "\n",
    "        aper_phot['aperture_sum_bgsub'] = aper_phot['aperture_sum'] - bkg_total\n",
    "        aper_phot['aperture_radius'] = radius\n",
    "        aper_phot.meta.clear()  # <-- Add this to avoid meta conflicts\n",
    "        \n",
    "        all_results.append(aper_phot)\n",
    "\n",
    "    result_table = vstack(all_results)\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def plot_radial_profile(\n",
    "    aperture_photometry_data,\n",
    "    output_filename=\"radial_profile.png\",\n",
    "    sky_radius_in=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the radial profile of background-subtracted aperture photometry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    aperture_photometry_data : astropy.table.Table\n",
    "        Table containing aperture photometry results with multiple aperture radii.\n",
    "        Must include columns: 'aperture_sum_bgsub', 'aperture_radius', and 'id' (if multiple sources).\n",
    "    output_filename : str\n",
    "        Path to the output plot image file.\n",
    "    sky_radius_in : float or None\n",
    "        Radius at which the sky annulus begins. If provided, a vertical line is plotted at this radius.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure input is an Astropy Table\n",
    "    if not isinstance(aperture_photometry_data, Table):\n",
    "        raise TypeError(\"Input must be an astropy.table.Table.\")\n",
    "\n",
    "    # Group data by source if 'id' column is available\n",
    "    if 'id' in aperture_photometry_data.colnames:\n",
    "        groups = aperture_photometry_data.group_by('id').groups\n",
    "        for group in groups:\n",
    "            source_id = group['id'][0]\n",
    "            radii = group['aperture_radius']\n",
    "            fluxes = group['aperture_sum_bgsub']\n",
    "            plt.plot(radii, fluxes, marker='o', label=f\"Source {source_id}\")\n",
    "    else:\n",
    "        # Assume single target\n",
    "        radii = aperture_photometry_data['aperture_radius']\n",
    "        fluxes = aperture_photometry_data['aperture_sum_bgsub']\n",
    "        plt.plot(radii, fluxes, marker='o', label=\"Target\")\n",
    "\n",
    "    # Plot vertical line for sky annulus inner radius\n",
    "    if sky_radius_in is not None:\n",
    "        plt.axvline(sky_radius_in, color='gray', linestyle='--', label='Sky Inner Radius')\n",
    "\n",
    "    plt.xlabel(\"Aperture Radius (pixels)\")\n",
    "    plt.ylabel(\"Background-subtracted Flux\")\n",
    "    plt.title(\"Radial Profile\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9cb57c3-6218-4c98-8c68-3ecf56100e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "\n",
    "def calculate_gain(files):\n",
    "    \"\"\"\n",
    "    Calculate the detector gain (e-/ADU) using two flat-field images.\n",
    "\n",
    "    Parameters:\n",
    "        files (list): List of two flat-field FITS file paths.\n",
    "\n",
    "    Returns:\n",
    "        float: Calculated gain in e-/ADU.\n",
    "    \"\"\"\n",
    "    if len(files) != 2:\n",
    "        raise ValueError(\"You must provide exactly two flat-field images.\")\n",
    "\n",
    "    # Read the flat-field images\n",
    "    flat1 = fits.getdata(files[0]).astype('f4')\n",
    "    flat2 = fits.getdata(files[1]).astype('f4')\n",
    "\n",
    "    # Calculate mean and variance\n",
    "    mean1 = np.mean(flat1)\n",
    "    mean2 = np.mean(flat2)\n",
    "    mean_comb = (mean1 + mean2) / 2.0\n",
    "\n",
    "    diff = flat1 - flat2\n",
    "    variance_diff = np.var(diff) / 2.0  # Because var(A - B) = 2 * var(single image)\n",
    "\n",
    "    gain = mean_comb / variance_diff\n",
    "\n",
    "    return float(gain)\n",
    "\n",
    "def calculate_readout_noise(files, gain):\n",
    "    \"\"\"\n",
    "    Calculate the readout noise in electrons (e-) using two bias frames.\n",
    "\n",
    "    Parameters:\n",
    "        files (list): List of two bias FITS file paths.\n",
    "        gain (float): Gain in e-/ADU.\n",
    "\n",
    "    Returns:\n",
    "        float: Readout noise in electrons.\n",
    "    \"\"\"\n",
    "    if len(files) != 2:\n",
    "        raise ValueError(\"You must provide exactly two bias frames.\")\n",
    "\n",
    "    # Read the bias frames\n",
    "    bias1 = fits.getdata(files[0]).astype(np.float32)\n",
    "    bias2 = fits.getdata(files[1]).astype(np.float32)\n",
    "\n",
    "    # Calculate variance of the difference\n",
    "    diff = bias1 - bias2\n",
    "    variance_diff = np.var(diff) / 2.0  # Same reasoning as above\n",
    "\n",
    "    readout_noise = np.sqrt(variance_diff) * gain\n",
    "\n",
    "    return float(readout_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93458c54-2a09-43b8-bc39-5cdc6c49cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_median_flats_per_filter(flat_files, bias_file, dark_file=None, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Create median flats grouped by filter.\n",
    "\n",
    "    flat_files: list of flat FITS file paths\n",
    "    bias_file: path to median bias FITS\n",
    "    dark_file: optional median dark FITS path\n",
    "    output_dir: where to save median flats\n",
    "    \n",
    "    Saves median flats named median_flat_<filter>.fits\n",
    "    \"\"\"\n",
    "    # Load bias and dark data once\n",
    "    bias_data = fits.getdata(bias_file)\n",
    "    dark_data = fits.getdata(dark_file) if dark_file else None\n",
    "    \n",
    "    # Group flats by FILTER header\n",
    "    flats_by_filter = defaultdict(list)\n",
    "    for f in flat_files:\n",
    "        with fits.open(f) as hdul:\n",
    "            filter_name = hdul[0].header.get(\"FILTER\", \"UNKNOWN\")\n",
    "        flats_by_filter[filter_name].append(f)\n",
    "    \n",
    "    for filter_name, files in flats_by_filter.items():\n",
    "        print(f\"Processing {len(files)} flats for filter {filter_name}\")\n",
    "        calibrated_flats = []\n",
    "        for fname in files:\n",
    "            flat_data = fits.getdata(fname)\n",
    "            # Bias subtract\n",
    "            flat_corr = flat_data - bias_data\n",
    "            # Dark subtract if available\n",
    "            if dark_data is not None:\n",
    "                flat_corr -= dark_data\n",
    "            calibrated_flats.append(flat_corr)\n",
    "        \n",
    "        # Stack and median combine\n",
    "        stack = np.array(calibrated_flats)\n",
    "        median_flat = np.median(stack, axis=0)\n",
    "        \n",
    "        # Normalize\n",
    "        median_val = np.median(median_flat)\n",
    "        if median_val == 0:\n",
    "            raise ValueError(f\"Median of flat for filter {filter_name} is zero\")\n",
    "        median_flat /= median_val\n",
    "        \n",
    "        # Save\n",
    "        outname = os.path.join(output_dir, f\"median_flat_{filter_name}.fits\")\n",
    "        fits.PrimaryHDU(data=median_flat).writeto(outname, overwrite=True)\n",
    "        print(f\"Saved median flat for filter {filter_name} to {outname}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe48e6ef-dd13-4dc0-ab33-71d02dd9d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage after bias and dark median files are created:\n",
    "\n",
    "data_dir = Path(\"astr-480-env/work/arcsat_data/scripts/data\")  # adjust to your folder with flats and other files\n",
    "\n",
    "# List all flat files (adjust pattern if needed)\n",
    "flat_files = sorted([str(f) for f in data_dir.glob(\"domeflat_*.fit\")])\n",
    "\n",
    "# Paths to your calibration files (already created)\n",
    "median_bias_file = data_dir / \"median_bias.fits\"\n",
    "median_dark_file = data_dir / \"median_dark.fits\"\n",
    "\n",
    "# Call the function to create median flats per filter\n",
    "create_median_flats_per_filter(\n",
    "    flat_files=flat_files,\n",
    "    bias_file=str(median_bias_file),\n",
    "    dark_file=str(median_dark_file),\n",
    "    output_dir=str(data_dir)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1cd0e23-3bc9-4dd0-81ab-8b64462ada97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias files: ['astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025309.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025324.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025336.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025348.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025401.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025413.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025425.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025437.fits', 'astr-480-env/work/arcsat_data/scripts/data/Bias_BIN1_20250601_025449.fits']\n",
      "Processing 7 flats for filter H-Alpha\n",
      "Saved median flat for filter H-Alpha to astr-480-env/work/arcsat_data/scripts/data/median_flat_H-Alpha.fits\n",
      "Processing 7 flats for filter g\n",
      "Saved median flat for filter g to astr-480-env/work/arcsat_data/scripts/data/median_flat_g.fits\n",
      "Processing 7 flats for filter i\n",
      "Saved median flat for filter i to astr-480-env/work/arcsat_data/scripts/data/median_flat_i.fits\n",
      "Processing 7 flats for filter r\n",
      "Saved median flat for filter r to astr-480-env/work/arcsat_data/scripts/data/median_flat_r.fits\n",
      "Processing 7 flats for filter z\n",
      "Saved median flat for filter z to astr-480-env/work/arcsat_data/scripts/data/median_flat_z.fits\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'PosixPath' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     30\u001b[39m reduce_science_frame(\n\u001b[32m     31\u001b[39m     science_filename=sci_files[\u001b[32m0\u001b[39m],\n\u001b[32m     32\u001b[39m     median_bias_filename=\u001b[38;5;28mstr\u001b[39m(data_dir / \u001b[33m\"\u001b[39m\u001b[33mmedian_bias.fits\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     reduced_science_filename = \u001b[38;5;28mstr\u001b[39m(data_dir / (\u001b[33m\"\u001b[39m\u001b[33mreduced_\u001b[39m\u001b[33m\"\u001b[39m + Path(sci_files[\u001b[32m0\u001b[39m]).name))\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 📌 Step 3: Show result\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fits.open(\u001b[38;5;28mstr\u001b[39m(\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreduced_\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msci_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m)) \u001b[38;5;28;01mas\u001b[39;00m hdul:\n\u001b[32m     40\u001b[39m     plt.imshow(hdul[\u001b[32m0\u001b[39m].data, cmap=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m, origin=\u001b[33m'\u001b[39m\u001b[33mlower\u001b[39m\u001b[33m'\u001b[39m, vmin=\u001b[32m0\u001b[39m, vmax=np.percentile(hdul[\u001b[32m0\u001b[39m].data, \u001b[32m99\u001b[39m))\n\u001b[32m     41\u001b[39m     plt.title(\u001b[33m\"\u001b[39m\u001b[33mReduced Science Frame\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'PosixPath' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 📍 Define your data directory\n",
    "data_dir = Path(\"astr-480-env/work/arcsat_data/scripts/data\")  # Adjust path depending on location of .fit files\n",
    "\n",
    "# 📌 Step 1: Make your calibration frames\n",
    "bias_files = sorted([str(f) for f in data_dir.glob(\"Bias_*.fits\")])\n",
    "dark_files = sorted([str(f) for f in data_dir.glob(\"Dark_*.fits\")])\n",
    "flat_files = sorted([str(f) for f in data_dir.glob(\"domeflat_*.fits\")])\n",
    "sci_files = sorted([str(f) for f in data_dir.glob(\"Cats_*.fits\")])\n",
    "print(\"Bias files:\", bias_files)\n",
    "\n",
    "\n",
    "create_median_bias(bias_files, str(data_dir / \"median_bias.fits\"))\n",
    "create_median_dark(dark_files, str(data_dir / \"median_bias.fits\"), str(data_dir / \"median_dark.fits\"))\n",
    "create_median_flats_per_filter(\n",
    "    flat_files=flat_files,\n",
    "    bias_file=str(median_bias_file),\n",
    "    dark_file=str(median_dark_file),\n",
    "    output_dir=str(data_dir)\n",
    ")\n",
    "\n",
    "\n",
    "median_flats = sorted(data_dir.glob(\"median_flat_*.fits\"))\n",
    "\n",
    "# 📌 Step 2: Reduce one science frame\n",
    "reduce_science_frame(\n",
    "    science_filename=sci_files[0],\n",
    "    median_bias_filename=str(data_dir / \"median_bias.fits\"),\n",
    "    median_dark_filename=str(data_dir / \"median_dark.fits\"),\n",
    "    median_flat_filename=str(median_flats[0]),\n",
    "    reduced_science_filename = str(data_dir / (\"reduced_\" + Path(sci_files[0]).name))\n",
    ")\n",
    "\n",
    "# 📌 Step 3: Show result\n",
    "with fits.open(data_dir / f\"reduced_{Path(sci_files[0]).name}\") as hdul:\n",
    "    plt.imshow(hdul[0].data, cmap='gray', origin='lower', vmin=0, vmax=np.percentile(hdul[0].data, 99))\n",
    "    plt.title(\"Reduced Science Frame\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e7bd3c7-cfd6-473e-9243-dcba09b7ec1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_001.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_002.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_003.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_004.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_005.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_006.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_H-Alpha_007.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_001.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_002.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_003.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_004.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_005.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_006.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_g_007.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_001.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_002.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_003.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_004.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_005.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_006.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_i_007.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_001.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_002.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_003.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_004.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_005.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_006.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_r_007.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_001.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_002.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_003.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_004.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_005.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_006.fits 2105280\n",
      "astr-480-env/work/arcsat_data/scripts/data/domeflat_z_007.fits 2105280\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in flat_files:\n",
    "    print(f, os.path.getsize(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d21d3-bb43-48de-a9e7-a7b2951557fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
    "\n",
    "sys.path.append('./scripts')\n",
    "\n",
    "data_dir = Path(\".\")\n",
    "filters = ['g', 'r', 'i', 'z', 'Halpha']  # Update to match your actual filters\n",
    "\n",
    "bias_files = sorted(data_dir.glob(\"Bias-*.fit\"))\n",
    "dark_files = sorted(data_dir.glob(\"Dark-*.fit\"))\n",
    "create_median_bias([str(f) for f in bias_files], \"median_bias.fits\")\n",
    "create_median_dark([str(f) for f in dark_files], \"median_bias.fits\", \"median_dark.fits\")\n",
    "\n",
    "for filt in filters:\n",
    "    print(f\"\\n--- Processing Filter: {filt} ---\")\n",
    "    \n",
    "    flat_files = sorted(data_dir.glob(f\"domeflat-{filt}-*.fit\"))\n",
    "    sci_files = sorted(data_dir.glob(f\"Cats-{filt}-*.fit\"))\n",
    "    \n",
    "    if not flat_files or not sci_files:\n",
    "        print(f\"⚠️ Missing files for filter {filt}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    flat_name = f\"median_flat_{filt}.fits\"\n",
    "    create_median_flat([str(f) for f in flat_files], \"median_bias.fits\", flat_name, dark_filename=\"median_dark.fits\")\n",
    "\n",
    "    for sci in sci_files:\n",
    "        reduced_name = f\"reduced_{sci.name}\"\n",
    "        reduce_science_frame(str(sci), \"median_bias.fits\", \"median_dark.fits\", flat_name, reduced_name)\n",
    "        print(f\"✅ Reduced: {reduced_name}\")\n",
    "\n",
    "        # Simple photometry on first image per filter\n",
    "        if sci == sci_files[0]:\n",
    "            with fits.open(reduced_name) as hdul:\n",
    "                data = hdul[0].data\n",
    "\n",
    "            plt.figure(figsize=(7, 6))\n",
    "            plt.imshow(data, origin='lower', cmap='gray', vmin=np.percentile(data, 5), vmax=np.percentile(data, 99))\n",
    "            plt.title(f\"Cat's Eye - Filter: {filt}\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "            positions = [(100, 100)]  # placeholder; refine this with DS9\n",
    "            ap = CircularAperture(positions, r=6)\n",
    "            ann = CircularAnnulus(positions, r_in=8, r_out=12)\n",
    "            mask = ann.to_mask(method='exact')[0]\n",
    "            ann_data = mask.multiply(data)\n",
    "            ann_1d = ann_data[mask.data > 0]\n",
    "            bkg_median = np.median(ann_1d)\n",
    "\n",
    "            phot = aperture_photometry(data, ap)\n",
    "            phot['bkg_subtracted'] = phot['aperture_sum'] - bkg_median * ap.area\n",
    "\n",
    "            print(f\"📊 Filter: {filt} Photometry\")\n",
    "            print(phot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c400e9da-1e35-4364-984f-485aefb1e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CCD Reduction Pipeline...\n",
      "\n",
      "Found 0 bias files\n",
      "Found 0 dark files\n",
      "Found 0 flat files\n",
      "Found 0 science files\n",
      "Creating median bias...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "data object array(nan) should have at least one dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_reduction\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_reduction\u001b[39m\u001b[34m(data_dir)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating median bias...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m median_bias_path = data_dir / \u001b[33m\"\u001b[39m\u001b[33mmedian_bias.fits\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mcreate_median_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmedian_bias_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating median dark...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m median_dark_path = data_dir / \u001b[33m\"\u001b[39m\u001b[33mmedian_dark.fits\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcreate_median_bias\u001b[39m\u001b[34m(bias_list, median_bias_filename)\u001b[39m\n\u001b[32m     33\u001b[39m     median_bias = median_bias.filled()\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# returns fits immages \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m primary = \u001b[43mfits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPrimaryHDU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmedian_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHeader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m hdul = fits.HDUList([primary])\n\u001b[32m     37\u001b[39m hdul.writeto(median_bias_filename, overwrite=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/hdu/image.py:1143\u001b[39m, in \u001b[36mPrimaryHDU.__init__\u001b[39m\u001b[34m(self, data, header, do_not_scale_image_data, ignore_blank, uint, scale_back)\u001b[39m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1098\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1099\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1104\u001b[39m     scale_back=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1105\u001b[39m ):\n\u001b[32m   1106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1107\u001b[39m \u001b[33;03m    Construct a primary HDU.\u001b[39;00m\n\u001b[32m   1108\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1141\u001b[39m \u001b[33;03m        (default: None)\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_not_scale_image_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_not_scale_image_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43muint\u001b[49m\u001b[43m=\u001b[49m\u001b[43muint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_blank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_blank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscale_back\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale_back\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;66;03m# insert the keywords EXTEND\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/hdu/image.py:155\u001b[39m, in \u001b[36m_ImageBaseHDU.__init__\u001b[39m\u001b[34m(self, data, header, do_not_scale_image_data, uint, scale_back, ignore_blank, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# Setting data will update the header and set _bitpix, _bzero,\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# and _bscale to the appropriate BITPIX for the data, and always\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# sets _bzero=0 and _bscale=1.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m = data\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Check again for BITPIX/BSCALE/BZERO in case they changed when the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# data was assigned. This can happen, for example, if the input\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# data is an unsigned int numpy array.\u001b[39;00m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28mself\u001b[39m._bitpix = \u001b[38;5;28mself\u001b[39m._header.get(\u001b[33m\"\u001b[39m\u001b[33mBITPIX\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/utils/decorators.py:862\u001b[39m, in \u001b[36mlazyproperty.__set__\u001b[39m\u001b[34m(self, obj, val)\u001b[39m\n\u001b[32m    860\u001b[39m obj_dict = obj.\u001b[34m__dict__\u001b[39m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fset:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m obj_dict.get(\u001b[38;5;28mself\u001b[39m._key) \u001b[38;5;129;01mis\u001b[39;00m ret:\n\u001b[32m    864\u001b[39m         \u001b[38;5;66;03m# By returning the value set the setter signals that it\u001b[39;00m\n\u001b[32m    865\u001b[39m         \u001b[38;5;66;03m# took over setting the value in obj.__dict__; this\u001b[39;00m\n\u001b[32m    866\u001b[39m         \u001b[38;5;66;03m# mechanism allows it to override the input value\u001b[39;00m\n\u001b[32m    867\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/hdu/image.py:279\u001b[39m, in \u001b[36m_ImageBaseHDU.data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    274\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    275\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m could not be coerced into an ndarray\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    276\u001b[39m             )\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape == ():\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    280\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m should have at least one dimension\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m         )\n\u001b[32m    283\u001b[39m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = data\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m._modified = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: data object array(nan) should have at least one dimension"
     ]
    }
   ],
   "source": [
    "run_reduction (data_dir = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89075b9d-1e7b-49dd-885c-bacdf1f9f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in data dir:\n",
      "Dark_BIN1_20250601_025555.fits\n",
      "Bias_BIN1_20250601_025348.fits\n",
      "Bias_BIN1_20250601_025309.fits\n",
      "domeflat_i_005.fits\n",
      "Cats_Eye_Nebula_g_20250601_065904.fits\n",
      "Dark_BIN1_20250601_025635.fits\n",
      "domeflat_z_003.fits\n",
      "Cats_Eye_Nebula_r_20250601_065842.fits\n",
      "domeflat_r_007.fits\n",
      "domeflat_r_006.fits\n",
      "domeflat_i_004.fits\n",
      "domeflat_g_007.fits\n",
      "Cats_Eye_Nebula_z_20250601_065953.fits\n",
      "Dark_BIN1_20250601_025609.fits\n",
      "Dark_BIN1_20250601_025622.fits\n",
      "Dark_BIN1_20250601_025516.fits\n",
      "Bias_BIN1_20250601_025437.fits\n",
      "Cats_Eye_Nebula_i_20250601_065929.fits\n",
      "domeflat_z_002.fits\n",
      "Dark_BIN1_20250601_025529.fits\n",
      "domeflat_z_005.fits\n",
      "domeflat_H-Alpha_005.fits\n",
      "Bias_BIN1_20250601_025401.fits\n",
      "Dark_BIN1_20250601_025543.fits\n",
      "Bias_BIN1_20250601_025425.fits\n",
      "Bias_BIN1_20250601_025449.fits\n",
      "Bias_BIN1_20250601_025413.fits\n",
      "Cats_Eye_Nebula_H-Alpha_20250601_070015.fits\n",
      "Dark_BIN1_20250601_025503.fits\n",
      "Bias_BIN1_20250601_025336.fits\n",
      ".ipynb_checkpoints\n",
      "Dark_BIN1_20250601_025648.fits\n",
      "domeflat_H-Alpha_004.fits\n",
      "Bias_BIN1_20250601_025324.fits\n",
      "domeflat_g_006.fits\n",
      "Bias files found: []\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"astr-480-env/work/arcsat_data/scripts/data\")  # change this to your actual path\n",
    "all_files = list(data_dir.iterdir())\n",
    "print(\"All files in data dir:\")\n",
    "for f in all_files:\n",
    "    print(f.name)\n",
    "\n",
    "bias_files = sorted([str(f) for f in data_dir.glob('Bias_*.fit')])\n",
    "print(\"Bias files found:\", bias_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f369d-d9fa-4d4c-ad5f-2bbde931cd62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
